{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a custom retrieval method based on term statistics from Elasticsearch\n",
    "\n",
    "  1. Implement the TF-IDF weighting formula by completing the `score()` method.\n",
    "      \n",
    "  2. Implement Language Modeling (query (log)likelihood) scoring as a new `score_lm()` method.\n",
    "  \n",
    "  3. Implement BM25 scoring as a new `score_bm25()` method.\n",
    "\n",
    "\n",
    "We use documents by taking the dot product of query and document term weights: $score(q,d)=\\sum_{t \\in q}w_{t,q} w_{t,d}$.\n",
    "Different retrieval models can be instantiated by setting these weights as follows:\n",
    "\n",
    "| Retrieval Model | $w_{t,q}$ | $w_{t,d}$ |\n",
    "| -- | -- | -- |\n",
    "| TF-IDF | $f_{t,q}$ | $\\frac{f_{t,d}}{|d|} IDF_t$ |\n",
    "| LM | $f_{t,q}$ | $\\log \\Big( (1-\\lambda) \\frac{f_{t,d}}{|d|} + \\lambda \\frac{f_{t,C}}{cl} \\Big)$ |\n",
    "| BM25 | $f_{t,q}$ | $\\frac{f_{t,d} (1+k_1)}{f_{t,d} + k_1(1-b+b\\frac{|d|}{avgdl})} \\times IDF_t$ |\n",
    "\n",
    " - $f_{t,q}$ is the frequency of term t in query q\n",
    " - $f_{t,d}$ is the frequency of term t in document d\n",
    " - $f_{t,C}$ is the frequency of term t in the entire collection\n",
    " - $IDF_{t}=\\log \\frac{N}{df_t}$\n",
    " - $N$ is the total number of documents in the collection\n",
    " - $df_t$ is the number of documents that contain term t\n",
    " - $|d|$ is the length of document d\n",
    " - $cl$ collection length (sum of all document lengths $\\sum_{d'}|d|$)\n",
    " - $\\lambda$ is a smoothing parameter\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INDEX_NAME = \"aquaint\"\n",
    "DOC_TYPE = \"doc\"\n",
    "FIELD = \"content\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring method\n",
    "\n",
    "This is our \"custom\" scoring method. \n",
    "  - `es` is an Elasticsearch object; this is needed for getting term statistics from Elasticsearch.\n",
    "  - `qterms` holds a sequence of query terms. It is important that these terms must be analyzed the same way documents were analyzed during indexing.\n",
    "  - `doc_id` is the document's ID.\n",
    "  \n",
    "The scoring method computes the dot product between the query term weights and document term weights: $score(q,d)=\\sum_{t \\in q}w_{t,q} w_{t,d}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(es, qterms, doc_id):\n",
    "    # Total number of documents in the index\n",
    "    n = es.count(index=INDEX_NAME, doc_type=DOC_TYPE)[\"count\"]\n",
    "\n",
    "    # Getting term frequency statistics for the given document field from Elasticsearch\n",
    "    tv = es.termvectors(index=INDEX_NAME, doc_type=DOC_TYPE, id=doc_id, fields=[FIELD],\n",
    "                              term_statistics=True).get(\"term_vectors\", {}).get(FIELD, {})\n",
    "    \n",
    "    # uncomment to see what information ES returns\n",
    "    #pprint.pprint(tv)\n",
    "        \n",
    "    dl = sum([s[\"term_freq\"] for t, s in tv[\"terms\"].items()])  # length of the document\n",
    "    cl = tv[\"field_statistics\"][\"sum_ttf\"]  # collection length (total number of terms in a given field in all documents)\n",
    "    avg_dl = cl / n\n",
    "    \n",
    "    print(avg_dl)\n",
    "    \n",
    "    s = 0  # holds the retrieval score\n",
    "    for t in qterms:\n",
    "        df_t = tv[\"terms\"][t][\"doc_freq\"]  # number of docs in the collection that contain that term\n",
    "        f_td = tv[\"terms\"][t][\"term_freq\"]  # raw frequenct of t in d (number of times term t appears in doc d)\n",
    "        t_tC = tv[\"terms\"][t][\"ttf\"]  # frequency of t in the entire collection\n",
    "        \n",
    "        # TODO: setting query and document term weights\n",
    "        w_tq = 1\n",
    "        w_td = 1\n",
    "        s += w_tq * w_td\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query analyzer\n",
    "\n",
    "See [indices.analyze](https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.client.IndicesClient.analyze)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_query(es, query):\n",
    "    tokens = es.indices.analyze(index=INDEX_NAME, body={\"text\": query})[\"tokens\"]\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x[\"position\"]):\n",
    "        query_terms.append(t[\"token\"])\n",
    "    return query_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"tropical storms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the top-1 document using Elasticsearch\n",
    "\n",
    "We search a single field (set in `FIELD`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = es.search(index=INDEX_NAME, q=query, df=FIELD, _source=False, size=1).get(\"hits\", {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the ID of the first hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APW19990810.0014\n"
     ]
    }
   ],
   "source": [
    "doc_id = res[\"hits\"][0][\"_id\"]\n",
    "print(doc_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-score this document using our own retrieval method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to transform the query string into a sequence of query terms, using the same analysis procedure that was used for building the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tropical', 'storms']\n"
     ]
    }
   ],
   "source": [
    "qterms = analyze_query(es, query)\n",
    "print(qterms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compute the \"custom\" retrieval score for this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423.6087138266466\n"
     ]
    }
   ],
   "source": [
    "new_score = score(es, qterms, doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(new_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
